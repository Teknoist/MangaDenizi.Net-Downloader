import os
import sys
import shutil
import asyncio
import aiohttp
import zipfile
import requests
from bs4 import BeautifulSoup
from lxml import html

# ANSI renk kodlarÄ± (terminal renkleri)
COLOR_GREEN = "\033[1;32m"
COLOR_BLUE = "\033[1;34m"
COLOR_RED = "\033[1;31m"
COLOR_RESET = "\033[0m"

# AYARLAR:
MAX_CONCURRENT_DOWNLOADS = 10  # AynÄ± anda indirilebilecek resim sayÄ±sÄ±

# URL'den baÅŸlamasÄ± gereken tÃ¼m bÃ¶lÃ¼m linklerini toplayan fonksiyon
def get_all_links(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = set()
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith(url):
                # URL'nin sonundaki '/' kaldÄ±rÄ±lÄ±p '/1' ekleniyor (sayfa numarasÄ±)
                full_link = href.rstrip('/') + '/1'
                links.add(full_link)
        return links
    except Exception as e:
        print(f"{COLOR_RED}âŒ get_all_links fonksiyonunda hata: {e}{COLOR_RESET}")
        return set()

# URL'leri bÃ¶lÃ¼m numarasÄ±na gÃ¶re sÄ±ralayan fonksiyon
def sort_urls(urls):
    def extract_chapter_number(url):
        try:
            # Ã–rnek URL: https://www.mangadenizi.net/manga/relife/051.5/1 â†’ bÃ¶lÃ¼m numarasÄ±: 51.5
            parts = url.strip().rstrip('/').split('/')
            chapter_str = parts[-2]
            return float(chapter_str)
        except Exception as e:
            print(f"{COLOR_RED}âŒ URL sÄ±ralama hatasÄ±: {e}{COLOR_RESET}")
            return float('inf')
    return sorted(urls, key=extract_chapter_number)

# Belirtilen klasÃ¶rdeki dosyalarÄ± zipleyip .cbz olarak kaydeden fonksiyon
def create_cbz(source_dir, output_filename):
    try:
        with zipfile.ZipFile(output_filename, 'w') as cbz:
            # DosyalarÄ± sÄ±ralÄ± bir ÅŸekilde arÅŸive ekliyoruz
            for root, _, files in os.walk(source_dir):
                for file in sorted(files):
                    file_path = os.path.join(root, file)
                    cbz.write(file_path, arcname=file)
        print(f"{COLOR_GREEN}ğŸ“¦ CBZ oluÅŸturuldu: {output_filename}{COLOR_RESET}")
    except Exception as e:
        print(f"{COLOR_RED}âŒ CBZ oluÅŸturulurken hata: {e}{COLOR_RESET}")

# Asenkron olarak tek bir resmin indirilip kaydedildiÄŸi ve yeniden denendiÄŸi fonksiyon
async def download_image(session, url, save_path, semaphore, max_retries=3):
    async with semaphore:
        for attempt in range(1, max_retries + 1):
            try:
                async with session.get(url) as resp:
                    if resp.status == 200:
                        content = await resp.read()
                        with open(save_path, 'wb') as f:
                            f.write(content)
                        print(f"{COLOR_GREEN}âœ… {os.path.basename(save_path)} indirildi! (Deneme {attempt}){COLOR_RESET}")
                        return True  # BaÅŸarÄ±lÄ± indirme
                    else:
                        print(f"{COLOR_RED}âŒ {os.path.basename(save_path)} HTTP {resp.status} ile indirilemedi (Deneme {attempt}).{COLOR_RESET}")
            except Exception as e:
                print(f"{COLOR_RED}âŒ {os.path.basename(save_path)} indirilirken hata: {e} (Deneme {attempt}){COLOR_RESET}")
            # Denemeler arasÄ±nda 1 saniye bekle
            await asyncio.sleep(1)
        return False  # TÃ¼m denemeler baÅŸarÄ±sÄ±z

# Bir bÃ¶lÃ¼mÃ¼n HTML sayfasÄ±nÄ± asenkron olarak Ã§ekip, resim linklerini ayrÄ±ÅŸtÄ±ran fonksiyon
async def fetch_chapter_data(chapter_url, session):
    try:
        async with session.get(chapter_url) as resp:
            if resp.status != 200:
                print(f"{COLOR_RED}âŒ {chapter_url} sayfasÄ± HTTP {resp.status} ile aÃ§Ä±lamadÄ±.{COLOR_RESET}")
                return None, []
            html_text = await resp.text()
            tree = html.fromstring(html_text)
            xpath_expression = '//img[@class="img-responsive"]/@data-src'
            raw_image_links = tree.xpath(xpath_expression)
            image_links = [
                ('https:' + link.strip()) if link.strip().startswith("//") else link.strip()
                for link in raw_image_links
            ]
            # BÃ¶lÃ¼m adÄ±nÄ± URL'den alÄ±yoruz (sondan bir Ã¶nceki kÄ±sÄ±m)
            chapter_name = chapter_url.rstrip('/').split('/')[-2]
            print(f"{COLOR_BLUE}ğŸ” {chapter_name} bÃ¶lÃ¼mÃ¼ alÄ±ndÄ±. ({len(image_links)} resim){COLOR_RESET}")
            return chapter_name, image_links
    except Exception as e:
        print(f"{COLOR_RED}âŒ {chapter_url} bÃ¶lÃ¼m verisi alÄ±nÄ±rken hata: {e}{COLOR_RESET}")
        return None, []

# Bir bÃ¶lÃ¼mÃ¼ indirip, eksik resimler varsa temizleyerek yeniden denemeye Ã§alÄ±ÅŸan fonksiyon.
async def process_chapter(chapter_name, image_links, max_chapter_retries=3):
    expected_count = len(image_links)
    for attempt in range(1, max_chapter_retries + 1):
        # GeÃ§ici klasÃ¶r: her deneme iÃ§in ayrÄ± bir klasÃ¶r kullanÄ±yoruz
        chapter_temp_dir = os.path.join("chapters", f"{chapter_name}_temp")
        os.makedirs(chapter_temp_dir, exist_ok=True)
        print(f"{COLOR_BLUE}ğŸš€ {chapter_name} indiriliyor... (Deneme {attempt}/{max_chapter_retries}){COLOR_RESET}")

        semaphore = asyncio.Semaphore(MAX_CONCURRENT_DOWNLOADS)
        connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT_DOWNLOADS * 2)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = []
            for idx, url in enumerate(image_links, start=1):
                file_ext = url.split('.')[-1] if '.' in url else 'jpg'
                filename = f"{idx:03d}.{file_ext}"
                save_path = os.path.join(chapter_temp_dir, filename)
                task = asyncio.create_task(download_image(session, url, save_path, semaphore, max_retries=3))
                tasks.append(task)
            await asyncio.gather(*tasks)

        # Ä°ndirme tamamlandÄ±ktan sonra, inen dosya sayÄ±sÄ±nÄ± kontrol ediyoruz.
        downloaded_files = [
            f for f in os.listdir(chapter_temp_dir)
            if os.path.isfile(os.path.join(chapter_temp_dir, f))
        ]
        actual_count = len(downloaded_files)

        if actual_count == expected_count:
            print(f"{COLOR_GREEN}âœ… {chapter_name} bÃ¶lÃ¼mÃ¼ tamamlandÄ±. {actual_count}/{expected_count} resim indirildi.{COLOR_RESET}")
            cbz_filename = os.path.join("chapters", f"{chapter_name}.cbz")
            create_cbz(chapter_temp_dir, cbz_filename)
            shutil.rmtree(chapter_temp_dir)
            return True
        else:
            print(f"{COLOR_RED}âš  {chapter_name} bÃ¶lÃ¼mÃ¼nde sorun: {actual_count} resim indirildi, {expected_count} bekleniyordu. Yeniden deneniyor...{COLOR_RESET}")
            shutil.rmtree(chapter_temp_dir)
    print(f"{COLOR_RED}âŒ {chapter_name} bÃ¶lÃ¼mÃ¼ iÃ§in maksimum deneme sayÄ±sÄ±na ulaÅŸÄ±ldÄ±. Bu bÃ¶lÃ¼m indirilemedi.{COLOR_RESET}")
    return False

# TÃ¼m bÃ¶lÃ¼mlerin verilerini asenkron olarak Ã§ekmek iÃ§in gÃ¶rev oluÅŸturan fonksiyon
async def fetch_all_chapter_data(chapter_urls):
    chapters_data = []
    connector = aiohttp.TCPConnector(limit=50)  # BÃ¶lÃ¼m Ã§ekme iÅŸlemlerinde eÅŸzamanlÄ±lÄ±k limiti artÄ±rÄ±ldÄ±
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [fetch_chapter_data(url, session) for url in chapter_urls]
        results = await asyncio.gather(*tasks)
        for chapter_name, image_links in results:
            if chapter_name is not None and image_links:
                chapters_data.append((chapter_name, image_links))
    return chapters_data

# Ana asenkron fonksiyon: TÃ¼m iÅŸlemleri organize eder
async def main():
    if len(sys.argv) != 2:
        print(f"{COLOR_BLUE}KullanÄ±m: python script.py <URL>{COLOR_RESET}")
        sys.exit(1)

    base_url = sys.argv[1]
    manga_adi = base_url.rstrip('/').split('/')[-1]
    print(f"{COLOR_BLUE}ğŸ“– {manga_adi} iÃ§in indirme iÅŸlemi baÅŸlatÄ±lÄ±yor...{COLOR_RESET}")

    # BÃ¶lÃ¼m URL'lerini Ã§ekiyoruz
    all_links = get_all_links(base_url)
    if not all_links:
        print(f"{COLOR_RED}âŒ HiÃ§bir bÃ¶lÃ¼m bulunamadÄ±!{COLOR_RESET}")
        sys.exit(1)

    # URL'leri bÃ¶lÃ¼m numarasÄ±na gÃ¶re sÄ±ralÄ±yoruz
    sorted_links = sort_urls(all_links)

    # TÃ¼m bÃ¶lÃ¼mlerin veri Ã§ekme iÅŸlemini asenkron olarak gerÃ§ekleÅŸtiriyoruz
    chapters_data = await fetch_all_chapter_data(sorted_links)
    if not chapters_data:
        print(f"{COLOR_RED}âŒ HiÃ§bir bÃ¶lÃ¼m verisi alÄ±namadÄ±!{COLOR_RESET}")
        sys.exit(1)

    # "chapters" klasÃ¶rÃ¼nÃ¼ oluÅŸturuyoruz (varsa kullanÄ±lÄ±r)
    os.makedirs("chapters", exist_ok=True)

    # Her bÃ¶lÃ¼mÃ¼ asenkron olarak iÅŸle
    tasks = [process_chapter(chapter_name, image_links) for chapter_name, image_links in chapters_data]
    await asyncio.gather(*tasks)

    # TÃ¼m CBZ dosyalarÄ± "chapters" klasÃ¶rÃ¼nde oluÅŸturulduktan sonra, manganÄ±n adÄ±yla bir klasÃ¶r oluÅŸturup oraya taÅŸÄ±yoruz.
    manga_klasoru = manga_adi  # Ã–rneÄŸin, "relife"
    os.makedirs(manga_klasoru, exist_ok=True)
    for file in os.listdir("chapters"):
        if file.endswith(".cbz"):
            src_path = os.path.join("chapters", file)
            dest_path = os.path.join(manga_klasoru, file)
            shutil.move(src_path, dest_path)

    print(f"{COLOR_GREEN}ğŸ {manga_adi} baÅŸarÄ±yla indirildi. TÃ¼m CBZ dosyalarÄ± '{manga_klasoru}' klasÃ¶rÃ¼nde saklanÄ±yor.{COLOR_RESET}")

if __name__ == "__main__":
    asyncio.run(main())
